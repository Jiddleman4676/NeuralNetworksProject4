\documentclass{article}

\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{positioning}

\geometry{
 a4paper,
 left=20mm,
 right=20mm,
 top=20mm,
 bottom=25mm,
}

\begin{document}

\begin{titlepage}
\begin{center}
\vspace*{1cm}
            
\Huge
\textbf{Midterm Project}
            
\vspace{1cm}

\Large
\text{Due: Sunday, October 27, 2024}

\vspace{2cm}

\text{\texttt{Brian High}} \\
\text{\texttt{Thomas Hynes}} \\
\text{\texttt{Jeremy Middleman}} \\
\text{\texttt{Andrei Phelps}} \\
\text{\texttt{Wayne Rudnick}} \\

\vspace{2cm}

\includegraphics[scale=0.25]{figs/icon.png}\\[0.5cm]

\vspace{9cm}

\textbf{CS 491/591: Neural Networks} \\

\end{center}
\end{titlepage}
\newpage

\section{Code Execution}

To run the code, run:
\[\text{python3 main.py}\]


Notes: 

Which specific cases are executed can be changed by changing the "True" and "False" flags in the main.py file.

The number of epochs is very important for each test. By default, we will have the number of epochs on the lower end so that the tests run quickly. However, if you want the tests to be more accurate, it is important that you change the number of epochs at the top of each of the three tests respectively. We still wanted to make the tests fairly accurate so there will be some waiting when you run main, just be patient and the graph will appear. 

The digits dataset can be run quickly (about 2 minutes) by setting the test size parameter in the train test split method to 0.99 with minimal loss in accuracy. Optimal accuracy was obtained when the test size parameter was set to 0.25, however this took a long time.


\section{Feedforward Neural Network (FNN)}

\subsection{Data Structures for FNN}

\subsubsection{\textbf{Layers}}
The FNN is composed of multiple layers stored as a list of layer objects within the network. Each layer object encapsulates essential information, including its weights, activation function, and gradient values, making it easy to process data through multiple layers.

\begin{itemize}
    \item \textbf{Weights}: Each layer contains weights stored as a matrix, initialized with random values. The weights include an additional column to accommodate the bias term, ensuring consistent results in the forward pass.
    \item \textbf{Activation Functions}: Each layer applies an activation function to the weighted input, introducing non-linearity into the model. The activations used include sigmoid, logsoftmax, and identity functions.
    \item \textbf{Gradients}: Gradients for each weight are accumulated in each layer, allowing the model to perform weight updates after each mini-batch.
\end{itemize}

\subsection{Forward and Backward Propagation}

\subsubsection{Forward Pass}
The forward pass involves feeding input data sequentially through each layer in the network. For each layer:
\begin{itemize}
    \item The layer calculates the dot product of its inputs and weights, then adds a bias term.
    \item The result is passed through an activation function, introducing non-linearity into the model.
    \item The activated output becomes the input for the next layer.
\end{itemize}
\begin{figure}[h!]
    \centering
    \resizebox{0.5\linewidth}{!}{ % Adjusts the size to half the linewidth while maintaining aspect ratio
    \begin{tikzpicture}

        % Define layer node style
        \tikzstyle{neuron} = [circle, draw, minimum size=0.8cm, node distance=1.5cm]

        % Input Layer
        \node[neuron, fill=blue!20] (input) at (0,0) {Input};

        % Hidden Layer (10 Neurons)
        \node[neuron, right=of input, xshift=4.5cm, yshift=4cm, fill=orange!20] (hidden1) {H1};
        \node[neuron, below of=hidden1] (hidden2) {H2};
        \node[neuron, below of=hidden2] (hidden3) {H3};
        \node[neuron, below of=hidden3] (hidden4) {H4};
        \node[neuron, below of=hidden4] (hidden5) {H5};
        \node[neuron, below of=hidden5] (hidden6) {H6};
        \node[neuron, below of=hidden6] (hidden7) {H7};
        \node[neuron, below of=hidden7] (hidden8) {H8};
        \node[neuron, below of=hidden8] (hidden9) {H9};
        \node[neuron, below of=hidden9] (hidden10) {H10};

        % Output Layer
        \node[neuron, fill=green!20] (output) at (10,0) {Output};

        % Draw arrows from Input Layer to Hidden Layer
        \foreach \i in {1,...,10}
            \draw[->] (input) -- (hidden\i);

        % Draw arrows from Hidden Layer to Output Layer
        \foreach \i in {1,...,10}
            \draw[->] (hidden\i) -- (output);

    \end{tikzpicture}
    }
    \caption{Data Flow Through the Network During the Forward Pass}
\end{figure}
During the forward pass, each input is passed through the network’s layers sequentially. Each layer computes the dot product of its inputs and weights, applies the activation function, and passes the result to the next layer.

\subsubsection{Backward Pass (Backpropagation)}
The backward pass, or backpropagation, calculates gradients for each layer’s weights, enabling the network to minimize the loss function. In this process:
\begin{itemize}
    \item The derivative of the loss function with respect to the network's output is computed.
    \item Each layer computes the gradient of its weights based on this derivative, the layer's inputs, and the activation function's derivative.
    \item These gradients are accumulated across the mini-batch and used to update the weights during gradient descent.
\end{itemize}

\section{Data Organization in Backpropagation}

Backpropagation is implemented using a gradient descent algorithm over mini-batches of data, which allows efficient updates after each batch. Gradients for the weights are computed by first calculating the derivative of the loss function with respect to the network’s output. Each layer then computes its weight gradients based on its inputs and these derivatives, storing the gradients in an accumulator for weight updates. Once a mini-batch is processed, the weights are updated based on the accumulated gradients.

\section{Experiments and Results}

\subsection{Sinusoidal Regression}
In the sinusoidal regression experiment, the FNN was trained to approximate the function $y = \sin(x)$ using a single hidden layer of 10 neurons. The network's approximation was observed to converge towards the sine function, indicating that the FNN effectively learned the relationship. Our tests showed that the results were almost perfect at about 1000,000 epochs.
\begin{figure}[h!]
    \centering
    % First row of images
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/FNN_Test01.png}
        \caption*{Approximation After 100 Epochs}
        \label{fig:100epochs}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/FNN_Test02.png}
        \caption*{Approximation After 1,000 Epochs}
        \label{fig:1000epochs}
    \end{subfigure}

    % Second row of images
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/FNN_Test03.png}
        \caption{Approximation After 10,000 Epochs}
        \label{fig:10000epochs}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/FNN_Test04.png}
        \caption*{Approximation After 100,000 Epochs}
        \label{fig:finalapproximation}
    \end{subfigure}
    \label{fig:sin_approximation}
\end{figure}
Some interesting results we found through testing was that the number of hidden layers had a larger impact than we expected in the overall fitting quality of the tests. When we had 4 or less hidden layers, we were never able to achieve a great fit. When we had 100+ hidden layers, the fitting got worse and worse. This is interesting as we found that there really is an ideal number of hidden layers and it is likely best found through trial and error. One thing that was always better for testing was increasing the number of epochs. The fitting on tests with a low number of epochs was never good. As the number of epochs increased, the test fitting got better and better. We found that higher learning rates were better when we have a low number of epochs, as you want to converge to a value faster. However, if you have the time to run more epochs, it is better to have a lower learning rate for precision.

\subsection{Data-Driven Model for ODE}
The data-driven model was tested using the Van der Pol oscillator’s differential equations. Two different differential equations were tested to compare the effects of varying the data. The results showed distinct patterns, as shown below.

Similar to the Sinusoidal Regression, we found that increasing the number of epochs seemed to greatly improve the accuracy of the prediction. We never found that increasing the number of epochs made the predictions worse. The number of hidden layers in this particular model seemed to have less of an impact than the Sinusoidal Regression. Similar to the previous test, we found that higher learning rates were better when we have a low number of epochs, as you want to converge to a value faster. In the case where you have the time to run more epochs, it is better to have a lower learning rate for precision as the prediction converges.

We also found that the ode from the given assignment provided a somewhat strange looking graph shape by default. So in our document we included the test from the vanderpol ode as we thought it was a better example. The code runs the ode from the assignment, and it can easily be changed in the "ode(underscore)model" method in main if you want to test with the ode from the vanderpol provided python code.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.4]{figs/Vander_Test01.png}
    \caption{Data-driven model using the ODE from the assignment (800 epochs).}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.4]{figs/Vander_Test02.png}
    \caption{Data-driven model using the Van der Pol ODE from the provided file (800 epochs).}
\end{figure}

\subsection{Handwritten Digit Classification}

The MNIST dataset was used to test the FNN on handwritten digit classification. Our model managed to achieve 98\% accuracy with the optimal combination of parameters (Fig. 5). 

One experiment that was done was to try to improve the time performance by reducing the number of neurons in the hidden layers and reducing the number of epochs. The number of epochs was reduced to ten and the number of neurons in each of the two inner hidden layers from 128 to 16. This did result in the program running faster, however the model predicted that all examples had value 1. Thus the model was unsuccessful at classifying the different digits. The model was unsuccessful with 30 epochs, as well. 84\% accuracy was achievable using only 1\% of the entire dataset (700 data samples) as part of training and 10 epochs, and ~91\% accuracy was achievable with (7000 data samples) and 84\% accuracy using 30 epochs and 1\% of the original data. Additionally, testing revealed that 128 is the optimal number of neurons in each of the two hidden layers. In our testing, a sigmoid function was used for all layers. When 256 neurons were in each of the two hidden layers, the accuracy was reduced to 74\% and down to 65\% when only 64 neurons were used in each layer. The optimal learning rate was 1.0. Learning rates above and below 1 yielded suboptimal accuracies.

These results show that the network achieved high accuracy, showing its capability to handle complex datasets and make reliable predictions. The key to getting this accuracy ended up being to optimize the FNN. With an inefficient implementation the training extremely long and would still jot result in good performance. After using the pycharm profiler, we were able to determine that the manual dotproduct, forward, and backward for-loops were dominating the training time. After switching these operations to the builtin numpy vector/matrix operation we ended up with about 100x improvement in runtime. This allowed us to run the entire 70000 data point data set through the FNN, which led to drastic improvements in accuracy and runtime.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{image.png}
    \caption{High accuracy is achievable using the FNN with the Handwritten digit classification dataset. Results shown are the result of using a neural network with the following layer sizes: 748, 128, 128, 10. A total of 30 epochs were run in the most accurate run of the model, which took 9 minutest to run.}
\end{figure}

\section{Individual Contributions}

\subsection{Brian High}
\begin{itemize}
    \item[1)] Helped to implement back propagation

\item[2)]Driver code for fetching the MNIST digits datset

\item[3)]Driver code for Training tesing on the digits data set

\item[4)]Plot for the digits classification 

\item[5)]Optimized FNN to use numpy matrix and vector built ins rather than loops

\item[6)]Optimized parameters for the digits dataset

\end{itemize}

\subsection{Thomas Hynes}
\begin{itemize}
    \item[1)] Worked on sections 1 and 2 of the project document.
    \item[2)] Wrote the gd and forward methods for the FFN.
    \item[3)] Modified and optimized other sections of the FFN and added comments.
\end{itemize}

\subsection{Jeremy Middleman}
\begin{itemize}
    \item[1)] Generated and analyzed data for the digits dataset and wrote this section discussing the data in the report.
    \item[2)] Adjusted the main class so that the digits dataset could be run through the model without it taking a long time.
    \item[3)] Wrote the Code Execution section of the report and helped organize main.py.
\end{itemize}

\subsection{Andrei Phelps}
\begin{itemize}
    \item[1)] Formatted, created, and organized the project document.
    \item[2)] Developed the \texttt{Layer} class in \texttt{layer.py}, implementing core methods for forward and backward propagation.
    \item[3)] Assisted in generating and refining the tests to validate the model's performance.
    \item[4)] Contributed to the development and debugging of the \texttt{main} and \texttt{FNN} classes.
\end{itemize}

\subsection{Wayne Rudnick}
\begin{itemize}
    \item[1)] Initialized the repository, added team-members, assigned tasks, and administered project progress
    \item[2)] Initialized the FNN and Layer class.
    \item[3)] Structured classes and methods to facilitate the transmission of appropriate data types. Integrated these classes and methods into tests 1-3.
    \item[4)] Created and completed Case 1 - Simple Regression
    \item[5)] Created and completed Case 2 - Data-Driven Model
    \item[6)] Ran the tests and optimized initial parameters. Reported on them in the final document here.
\end{itemize}

\end{document}
